{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pipeline\n",
    "\n",
    "Pipeline is a Python package to build complex analysis pipelines with [dask](https://dask.org).\n",
    "\n",
    "Inspired on spotify's [Luigi](https://github.com/spotify/luigi), Pipeline builds on top of `dask.delayed`, adding save and load instructions to the dask graph.\n",
    "\n",
    "In contrast to Luigi, Pipeline allows to experiment with the analysis pipeline more interactively, thanks to dask. For instance, within a Jupyter notebook.\n",
    "\n",
    "At the same time, it can also accelerate experimenting with the analysis by storing expensive intermediate computations to disk, in contrast to plain dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Task, Storage, dataclass, dependency, task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task decorator\n",
    "\n",
    "A Task can be created from a function using `task` as a decorator.\n",
    "\n",
    "If you're familiar with `dask.delayed`, this function is as if it were a wrapper of `dask.delayed(pure=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def double(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function doesn't perform the computation immediately, but returns a `dask.delayed` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('double/54abf7066a03bddb650fbb1616e27bc5.cloudpickle.zstandard')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the result, you must call the `.compute` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double(21).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing results to disk\n",
    "\n",
    "So far, a task behaves as a `dask.delayed`. But, additionally, we can easily save and load data to avoid recomputing expensive tasks.\n",
    "\n",
    "To do that, we first need to create a `Storage` object, which uses any object implementing a `MutableMapping[str, bytes]` interface.\n",
    "\n",
    "The constructor also accepts a `str`, and in that case builds a `fsspec.FSMap`. Here, we create a `fsspec.FSMap` dict-backed in-memory storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(\"memory://\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save (and then load) a Task, we must explicitly create it with `save=True`, as we probably don't want to save cheap to compute and expensive to store tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(save=True)\n",
    "def double_with_print(x: float) -> float:\n",
    "    out = 2 * x\n",
    "    print(f\"Calculating: 2 * {x} = {out}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe when the function is actually called, we added a `print` statement inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('double_with_print/54abf7066a03bddb650fbb1616e27bc5.cloudpickle.zstandard')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_task = double_with_print(21)\n",
    "\n",
    "my_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function, creates a `dask.delayed` object, as before.\n",
    "\n",
    "If we call `.compute()`, we see the print statement and it returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: 2 * 21 = 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we call compute inside an storage context manager, that result will be saved, and loaded in future computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: 2 * 21 = 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with storage(save=True):\n",
    "    result = my_task.compute()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we recompute the task inside the context manager, we obtain the same result, but we see no print statement, as the function wasn't called, but the result retrieved from storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with storage(save=True):\n",
    "    result = my_task.compute()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, recomputing outside the context manager calls the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: 2 * 21 = 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** functions are expected to be **pure**, that is, that their output only depend on their input parameters, and have no side effects. A function will not be called again when its output is already stored.\n",
    "\n",
    "Example of non-pure functions:\n",
    "\n",
    "- **Mutating input**: using in-place operations.\n",
    "- **Non-deterministic output**: drawing random numbers, or relying on global variables.\n",
    "- **Side effects:** updating global variables.\n",
    "\n",
    "We've seen an example of a side effect: the `print` function in the previous example was not called when recomputing the task (inside the storage context manager)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Storages\n",
    "\n",
    "There are two ways to combine storages:\n",
    "\n",
    "#### Chaining storages\n",
    "\n",
    "To join multiple storages, we can use `Storage.chain` to create a joint `MutableMapping` as follows:\n",
    "\n",
    "```python\n",
    "storage_local = Storage(\"/local_folder\")\n",
    "storage_remote = Storage(\"ssh://user@server/home/user/remote_folder\")\n",
    "storage_combined = Storage.from_chain(storage_local, storage_remote)\n",
    "\n",
    "with storage_combined(save=True):\n",
    "    ...\n",
    "```\n",
    "\n",
    "In this case, `storage` will try to load in order, first from `\"local_folder\"` and then from `\"remote_folder\"`. If it needs to save a task, it will be saved to the first one (`local_storage`).\n",
    "\n",
    "*Note: underneath it's just using a `collections.ChainMap` to join them.*\n",
    "\n",
    "#### Nested storages\n",
    "\n",
    "Alternatively, we can simply nest the context managers:\n",
    "\n",
    "```python\n",
    "with storage_remote(save=True):\n",
    "    # Tasks computed here load from and save to remote only\n",
    "    task.compute()\n",
    "\n",
    "    with storage_local(save=True):\n",
    "        # Tasks computed here load from local or remote (in that order)\n",
    "        # and save to local.\n",
    "        task.compute()\n",
    "\n",
    "    with storage_local(save=True, nested=False):\n",
    "        # Tasks computed here ignore remote\n",
    "        task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "With the task decorator, we can also set default values as default arguments in the decorated function. But, another feature of `pipeline` is the possibility of declaring default parameters which can be computed from its input parameters in other tasks.\n",
    "\n",
    "For that, we have to define our task as a subclass of `Task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass  # optional, but useful for type hints\n",
    "class NormalizedData(Task):\n",
    "    data: np.ndarray\n",
    "    center: dependency[float]\n",
    "    scale: dependency[float] = 1\n",
    "\n",
    "    @staticmethod  # optional, it is automatically converted to a staticmethod.\n",
    "    def run(data, center, scale) -> np.ndarray:\n",
    "        return (data - center) / scale\n",
    "\n",
    "    @dependency\n",
    "    def center(self) -> float:\n",
    "        return compute_center(self.data)\n",
    "\n",
    "\n",
    "@task(save=True)\n",
    "def compute_center(x: np.ndarray) -> float:\n",
    "    print(\"Computing center.\")\n",
    "    return np.median(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Task` use the same syntax as dataclasses: we specify its parameters as class annotations. For instance, `data: np.ndarray`.\n",
    "\n",
    "*Note: in fact, the `Task.__init__` method is built with `dataclass`, and we can decorate the class with `@dataclass` to obtain type hints.*\n",
    "\n",
    "We can set default values as class attributes, as is the case with `scale = 1`.\n",
    "\n",
    "We can define *dependencies* as methods without parameters (besides `self`), decorated with `@dependency`, as is the case with `center`.\n",
    "\n",
    "Finally, we need to define a `run` staticmethod, which will be what the task computes.\n",
    "\n",
    "*Note: the names of `run`'s parameters must be defined at least as a class annotation, class attribute or dependency method.*\n",
    "\n",
    "Let's use this task with a sample array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `NormalizedData` with an explicit value for `center`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 6., 7., 8.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalizedData(data, center=0).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can leave it unspecified, and it will be computed from the `compute_center` task for the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing center.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.5, -0.5,  0.5,  1.5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalizedData(data).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `compute_center` task was created with `save=True`, if we compute `NormalizedData` inside a storage context manager, the `compute_center` intermediate result will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing center.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.5, -0.5,  0.5,  1.5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with storage(save=True):\n",
    "    result = NormalizedData(data).compute()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we recompute `NormalizedData`, we see no `Computing center.` printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5, -0.5,  0.5,  1.5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with storage(save=True):\n",
    "    result = NormalizedData(data).compute()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is also true if we are interactively playing with the `scale` parameter, as it is the `compute_center` task that was stored, and it only depended on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3., -1.,  1.,  3.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with storage(save=True):\n",
    "    result = NormalizedData(data, scale=0.5).compute()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and decoding\n",
    "\n",
    "To store data, results need to be encoded into a bytes representation. By default, Task encodes data by serializing with `cloudpickle` and compressing with `zstandard`.\n",
    "\n",
    "But, `cloudpickle` is not appropriate for long-term storage, as it depends on the Python version used. We can easily customize the encoding by passing another serializer that implements the `dumps` and `loads` functions. To demonstrate this, we will use `json` and `yaml` from the [serialize](https://github.com/hgrecco/serialize) project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serialize import json, yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, we create an in-memory `Storage`, and a function which outputs a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(\"memory://\")\n",
    "storage.fs.clear()  # it is reuses the previous in-memory storage, so we clear it.\n",
    "\n",
    "\n",
    "def point_as_dict(x, y):\n",
    "    return dict(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create 3 Task variants from that function, with different serializers, and no compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set compressor=None to see that the bytes correspond to json/yaml serialized data.\n",
    "as_default = task(point_as_dict, save=True, compressor=None)\n",
    "as_json = task(point_as_dict, save=True, serializer=json, compressor=None)\n",
    "as_yaml = task(point_as_dict, save=True, serializer=yaml, compressor=None)\n",
    "\n",
    "with storage(save=True):\n",
    "    as_default(x=1, y=2).compute()\n",
    "    as_json(x=1, y=2).compute()\n",
    "    as_yaml(x=1, y=2).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the underlying \"files\" to see that it was properly serialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_as_dict/85985a7bd912508f89c96f0ff61e15a6.cloudpickle   b'\\x80\\x05\\x95\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x01x\\x94K\\x01\\x8c\\x01y\\x94K\\x02u.'\n",
      "point_as_dict/85985a7bd912508f89c96f0ff61e15a6.json          b'{\"x\": 1, \"y\": 2}'\n",
      "point_as_dict/85985a7bd912508f89c96f0ff61e15a6.yaml          b'x: 1\\ny: 2\\n'\n"
     ]
    }
   ],
   "source": [
    "for k, v in storage.fs.items():\n",
    "    print(f\"{k:<60} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom encoding\n",
    "\n",
    "To customize the encoding, the `Task` class has the following attributes:\n",
    "\n",
    "```python\n",
    "class Task:\n",
    "    encoders: Optional[tuple[Encoder]] = None\n",
    "    serializer: Optional[Serializer] = cloudpickle\n",
    "    compressor: Optional[Compressor] = zstandard\n",
    "    encrypter: Optional[Encrypter] = None\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "| type       | implements           |\n",
    "|------------|----------------------|\n",
    "| Encoder    | encode, decode       |\n",
    "| Serializer | dumps, loads         |\n",
    "| Compressor | compress, decompress |\n",
    "| Encrypter  | encrypt, decrypt     |\n",
    "\n",
    "and implements the default encoding of the output of `Task.run` as the following transformations:\n",
    "\n",
    "`encoders[0] -> ... -> encoder[-1] > serializer -> compressor -> encrypter`\n",
    "\n",
    "Most serializers, compressors and encrypters already implement this interface, so you can simply import a module/class/object and use it.\n",
    "\n",
    "### Even more custom encoding\n",
    "\n",
    "Before serializing, it might be useful to apply certain transformations to the result.\n",
    "\n",
    "For instance, `encoders` can be a tuple of [`numcodecs`](https://numcodecs.readthedocs.io/en/stable/) filters.\n",
    "\n",
    "But if no pre-built encoder exists, we can also customize the encoding further by overriding the `Task.encode` and `Task.decode` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_as_dict/85985a7bd912508f89c96f0ff61e15a6.cloudpickle   b'\\x80\\x05\\x95\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x01x\\x94K\\x01\\x8c\\x01y\\x94K\\x02u.'\n",
      "point_as_dict/85985a7bd912508f89c96f0ff61e15a6.json          b'{\"x\": 1, \"y\": 2}'\n",
      "point_as_dict/85985a7bd912508f89c96f0ff61e15a6.yaml          b'x: 1\\ny: 2\\n'\n",
      "point_as_dict_2/85985a7bd912508f89c96f0ff61e15a6.json        b'[1, 2]'\n"
     ]
    }
   ],
   "source": [
    "class point_as_dict_2(Task):\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "    run = point_as_dict\n",
    "    save = True\n",
    "    serializer = json\n",
    "    compressor = None\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Transform the dict to a tuple before calling encode.\"\"\"\n",
    "        x = (x[\"x\"], x[\"y\"])\n",
    "        return super().encode(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"Transform the tuple back to a dict.\"\"\"\n",
    "        x = super().decode(x)\n",
    "        return {\"x\": x[0], \"y\": x[1]}\n",
    "\n",
    "\n",
    "with storage(save=True):\n",
    "    point_as_dict_2(x=1, y=2).compute()\n",
    "\n",
    "for k, v in storage.fs.items():\n",
    "    print(f\"{k:<60} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we called `super().encode` to continue with the default encoding after transforming the dict to a tuple.\n",
    "\n",
    "Note: we had to change the Task name, adding a `_2` at the end. Otherwise, it would have collided with the previous `point_as_dict` serialized as JSON, as they had the same input parameters to run, producing the same hash `859...5a6`, and the same extension `.json`. We could also have assigned different names to the previous tasks using the name parameter of the `task` function: `task(as_dict, name=\"foo\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common pitfalls\n",
    "\n",
    "The result of a Task is saved according to a hash of its run arguments. Although two computations might produce the same result, they might be constructed differently and not loaded from storage. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(\"memory://\")\n",
    "storage.fs.clear()  # it is reuses the previous in-memory storage, so we clear it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a task to load an \"image\" and then another to compute its minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing min for [1 2 3]\n",
      "Computing min for [1 2 3]\n",
      "Computing min for [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "@task\n",
    "def load_image(file: str) -> np.ndarray:\n",
    "    return np.array([1, 2, 3])\n",
    "\n",
    "\n",
    "@task(save=True, serializer=json, compressor=None)\n",
    "def compute_min(image: np.ndarray) -> float:\n",
    "    print(\"Computing min for\", image)\n",
    "    return int((image).min())\n",
    "\n",
    "\n",
    "with storage(save=True):\n",
    "    # We create a task to load an image (but didn't load it yet)\n",
    "    # and pass it to the compute_min task, and compute it\n",
    "    image1 = load_image(\"image\")\n",
    "    min_task1 = compute_min(image1)\n",
    "    min_task1.compute()\n",
    "\n",
    "    # Now, we actually load the image before passing it to\n",
    "    # compute_min. The input is now a np.ndarray instead of\n",
    "    # dask.delayed\n",
    "    image2 = image1.compute()\n",
    "    min_task2 = compute_min(image2)\n",
    "    min_task2.compute()\n",
    "\n",
    "    # Here, even-though it will load the same array, it has\n",
    "    # a different argument, and represents a different image.\n",
    "    # Storage doesn't know a priori that it will result in\n",
    "    # the same argument.\n",
    "    image3 = load_image(\"image2\")\n",
    "    min_task3 = compute_min(image3)\n",
    "    min_task3.compute()\n",
    "\n",
    "    # Now, this was already computed before, and will be\n",
    "    # retrieved from storage\n",
    "    image4 = image3.compute()\n",
    "    min_task4 = compute_min(image4)\n",
    "    min_task4.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it was computed 3 times, even-though the actual computation received the same argument.\n",
    "\n",
    "To understand it, it can be useful to see the inputs that each `compute_min` received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1 = load_image(\"image\")  ->  Delayed('load_image/7fa16d227d7f86a4251029b028e07189.cloudpickle.zstandard')\n",
      "image2 = image1.compute()     ->  [1 2 3]\n",
      "image3 = load_image(\"image2\") ->  Delayed('load_image/623caa47f757c5946eb49415780e5565.cloudpickle.zstandard')\n",
      "image4 = image3.compute()     ->  [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print('image1 = load_image(\"image\")  -> ', image1)\n",
    "print(\"image2 = image1.compute()     -> \", image2)\n",
    "print('image3 = load_image(\"image2\") -> ', image3)\n",
    "print(\"image4 = image3.compute()     -> \", image4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even-though `image1` and `image3` will compute to the same, before computing each task looks different, because they have a different (and irrelevant in this example) argument.\n",
    "\n",
    "Hence, when passed to `compute_min`, it couldn't be recognized as the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_task1 = compute_min(image1)  ->  Delayed('compute_min/9d19fc947e66b340c9d5d676ca71544d.json')\n",
      "min_task2 = compute_min(image2)  ->  Delayed('compute_min/6df7d63420af1411f533b4050768e4a1.json')\n",
      "min_task3 = compute_min(image3)  ->  Delayed('compute_min/97f86c151b43416b51ca653094a8b6da.json')\n",
      "min_task4 = compute_min(image4)  ->  Delayed('compute_min/6df7d63420af1411f533b4050768e4a1.json')\n"
     ]
    }
   ],
   "source": [
    "print(\"min_task1 = compute_min(image1)  -> \", min_task1)\n",
    "print(\"min_task2 = compute_min(image2)  -> \", min_task2)\n",
    "print(\"min_task3 = compute_min(image3)  -> \", min_task3)\n",
    "print(\"min_task4 = compute_min(image4)  -> \", min_task4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, `min_task2` and `min_task4` received the same input, they ended up with the same hash, and `min_task4` was loaded from storage.\n",
    "\n",
    "We can check the storage to see that only 3 tasks were saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_min/6df7d63420af1411f533b4050768e4a1.json            b'1'\n",
      "compute_min/97f86c151b43416b51ca653094a8b6da.json            b'1'\n",
      "compute_min/9d19fc947e66b340c9d5d676ca71544d.json            b'1'\n"
     ]
    }
   ],
   "source": [
    "for k, v in storage.fs.items():\n",
    "    print(f\"{k:<60} {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: this example might suggest that it is wise to call `.compute` on task arguments beforehand, but it doesn't!.\n",
    "\n",
    "When trying to recompute:\n",
    "\n",
    "```python\n",
    "image = load_image(\"image\")\n",
    "compute_min(image).compute()\n",
    "```\n",
    "\n",
    "on the future, we won't have to actually load the image. Instead, we would have to load the image if we did:\n",
    "\n",
    "```python\n",
    "image = load_image(\"image\").compute()\n",
    "compute_min(image).compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read more at dask.org\n",
    "\n",
    "As `Task` works on top of `dask.delayed`, it is useful to check out dask's documentation:\n",
    "\n",
    "- Delayed: https://docs.dask.org/en/stable/delayed.html\n",
    "- Delayed Collections: https://docs.dask.org/en/stable/delayed-collections.html\n",
    "- Delayed Best Practices: https://docs.dask.org/en/stable/delayed-best-practices.html\n",
    "- General Best Practices: https://docs.dask.org/en/stable/best-practices.html\n",
    "\n",
    "Many tips discussed there apply for `Task` too. In particular, these points, which were discussed before:\n",
    "\n",
    "- Don’t mutate inputs\n",
    "- Avoid global state\n",
    "- Don’t rely on side effects"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53db425adbeb5274c539f12630014ccb52619270733afeaf3d2657c734123dfc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('pipeline': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
